{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-supervisor",
   "metadata": {},
   "source": [
    "## Simple NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units = 16, input_dim=2, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# State the loss function and optimizer (adam is a good choice usually)\n",
    "model.compile(optimizer=keras.optimizers.Adam(),loss='mean_squared_error',metrics=['accuracy'])\n",
    "\n",
    "losses = model.fit(X,y,epochs=500,verbose=2)\n",
    "\n",
    "# View improvement over epochs\n",
    "plt.plot(losses.history['loss'])\n",
    "plt.show()\n",
    "plt.plot(losses.history['accuracy']) # Notice we are able to plot the accuracy in training as well\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.predict(X) # Make predictions\n",
    "pred = (prob > 0.5) + 0\n",
    "#Evaluate the accuracy\n",
    "print(np.mean(pred == y)) # Accuracy\n",
    "# Since this is training data, compare with the final training accuracy\n",
    "print(losses.history['accuracy'][-1])\n",
    "print(confusion_matrix(y_pred=pred , y_true=y)) # Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to cell 2 and consider other network structures\n",
    "# What are the mathematical formulations for these structures?\n",
    "\n",
    "## In case you want to save or load a model\n",
    "#model.save('path/to/location')\n",
    "#reload_model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-observer",
   "metadata": {},
   "source": [
    "## Quadratic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nonlinear relationship\n",
    "N = 100\n",
    "x = np.random.normal(loc=0,scale=1,size=N)\n",
    "epsilon = np.random.normal(loc=0,scale=2,size=N)\n",
    "B0 = 2\n",
    "B2 = 3\n",
    "y = B0 + B2*x**2 + epsilon\n",
    "\n",
    "x_test = np.arange(start=min(x),stop=max(x),step=0.01)\n",
    "y_true = B0 + B2*x_test**2\n",
    "\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true,color='k',linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a linear regression\n",
    "xx = np.vstack([np.ones(len(x)),x]).T\n",
    "b0,b1 = np.linalg.lstsq(xx,y,rcond=None)[0]\n",
    "print([b0,b1])\n",
    "\n",
    "# Plot response\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true,color='k',linewidth=2)\n",
    "plt.axline((0,b0),slope=b1,color='r',linewidth=2)\n",
    "\n",
    "# Compute MSE\n",
    "print(np.mean(((b0+b1*x_test) - y_true)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quadratic regression (since we know this is the ground truth)\n",
    "xx = np.vstack([np.ones(len(x)),x,x**2]).T\n",
    "b0,b1,b2 = np.linalg.lstsq(xx,y,rcond=None)[0]\n",
    "print([b0,b1,b2])\n",
    "\n",
    "# Predict on test data\n",
    "y_quad = b0 + b1*x_test + b2*x_test**2\n",
    "\n",
    "# Plot response\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true,color='k',linewidth=2)\n",
    "plt.plot(x_test,y_quad,color='r',linewidth=2)\n",
    "\n",
    "# Compute MSE\n",
    "print(np.mean((y_quad - y_true)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a neural network\n",
    "# What is the shape of this network?\n",
    "HIDDEN = 16\n",
    "NN =keras.Sequential()\n",
    "NN.add(keras.layers.Dense(HIDDEN, input_dim=1,activation='relu')) #1 input x\n",
    "NN.add(keras.layers.Dense(1, activation='linear')) #1 output y\n",
    "\n",
    "NN.summary()\n",
    "\n",
    "# State the loss function and optimizer (adam is a good choice usually)\n",
    "NN.compile(optimizer=keras.optimizers.Adam(),loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to training data\n",
    "EPOCHS = 500 # How long to train for\n",
    "history = NN.fit(x,y,epochs=EPOCHS,verbose=0)\n",
    "# View improvement over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_NN = NN.predict(x_test)\n",
    "\n",
    "# Plot response\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true,color='k',linewidth=2)\n",
    "plt.plot(x_test,y_quad,color='r',linewidth=2)\n",
    "plt.plot(x_test,y_NN,color='b',linewidth=2)\n",
    "\n",
    "# Compute MSE\n",
    "print(np.mean((y_NN - y_true)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a *deep* neural network\n",
    "# What is the structure of this neural network?\n",
    "# Consider a neural network\n",
    "# What is the shape of this network?\n",
    "HIDDEN = 16\n",
    "NN =keras.Sequential()\n",
    "NN.add(keras.layers.Dense(HIDDEN, input_dim=1,activation='relu')) #1 input x\n",
    "NN.add(keras.layers.Dense(HIDDEN, activation='tanh'))\n",
    "NN.add(keras.layers.Dense(1, activation='linear')) #1 output y\n",
    "\n",
    "NN.summary()\n",
    "\n",
    "# State the loss function and optimizer (adam is a good choice usually)\n",
    "NN.compile(optimizer=keras.optimizers.Adam(),loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to training data\n",
    "EPOCHS = 1000 # How long to train for (let this train longer)\n",
    "history = NN.fit(x,y,epochs=EPOCHS,verbose=0)\n",
    "# View improvement over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_NN = NN.predict(x_test)\n",
    "\n",
    "# Plot response\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true,color='k',linewidth=2)\n",
    "plt.plot(x_test,y_quad,color='r',linewidth=2)\n",
    "plt.plot(x_test,y_NN,color='b',linewidth=2)\n",
    "\n",
    "# Compute MSE\n",
    "print(NN.evaluate(x_test, y_true, verbose=0)) # Another way to compute the MSE (losses)\n",
    "# Let's go back and consider other network structures\n",
    "# What are the mathematical formulations for these structures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NN.evaluate(x_test, y_true, verbose=0)) # Another way to compute the MSE (losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-google",
   "metadata": {},
   "source": [
    "## Quantile regression (changing the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the 10% quantile\n",
    "# Test/plot fits\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true+norm.ppf(0.1,scale=2),color='r',linewidth=2)\n",
    "\n",
    "# Run a quadratic quantile regression (since we know this is the ground truth)\n",
    "quad_reg = QuantileRegressor(quantile=0.1, alpha=0) #alpha is for a Lasso-type penalty term\n",
    "X = np.transpose(np.vstack((x,x**2)))\n",
    "quad_reg.fit(X, y)\n",
    "print((quad_reg.intercept_ , quad_reg.coef_))\n",
    "\n",
    "X_test = np.transpose(np.vstack((x_test,x_test**2)))\n",
    "y_quad_pred = quad_reg.predict(X_test)\n",
    "plt.plot(x_test,y_quad_pred,color='b',linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg = keras.Sequential()\n",
    "nn_reg.add(keras.layers.Dense(16, input_dim=1,activation='relu')) #1 input x\n",
    "nn_reg.add(keras.layers.Dense(16, activation='relu'))\n",
    "nn_reg.add(keras.layers.Dense(1, activation='linear')) #1 output y\n",
    "\n",
    "nn_reg.summary()\n",
    "\n",
    "# We need to create our own loss function\n",
    "import keras.backend as K\n",
    "def quantile_loss(q):\n",
    "    def ql(y_true , y_pred): \n",
    "        e = y_true - y_pred\n",
    "        #if e > 0 then qe > (q-1)e; if e < 0 then qe < (q-1)e\n",
    "        loss = K.mean(K.maximum(q*e , (q-1)*e)) \n",
    "        return loss\n",
    "    \n",
    "    return ql\n",
    "\n",
    "# State the loss function and optimizer (adam is a good choice usually)\n",
    "nn_reg.compile(optimizer=keras.optimizers.Adam(),loss=quantile_loss(0.1)) #Now we minimize our custom loss function\n",
    "\n",
    "# Fit the model to training data\n",
    "losses = nn_reg.fit(x,y,epochs=500,verbose=0) # Fit the model to training data\n",
    "# View improvement over epochs\n",
    "plt.plot(losses.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the neural network\n",
    "y_nn_pred = nn_reg.predict(x_test)\n",
    "plt.scatter(x,y,color='k')\n",
    "plt.plot(x_test,y_true+norm.ppf(0.1,scale=2),color='r',linewidth=2)\n",
    "plt.plot(x_test,y_quad_pred,color='b',linewidth=2)\n",
    "plt.plot(x_test,y_nn_pred,color='g',linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-wrestling",
   "metadata": {},
   "source": [
    "## Feedforward NN for financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download financial data:\n",
    "import yfinance\n",
    "from datetime import datetime\n",
    "\n",
    "#myData = DataReader([\"IBM\"],\"yahoo\",datetime(2010,1,1),datetime(2021,12,31)) #IBM chosen at random\n",
    "#IBM = myData[\"Adj Close\"][\"IBM\"]\n",
    "myData = yfinance.download([\"IBM\"],datetime(2010,1,1),datetime(2021,12,31))\n",
    "IBM = myData[\"Adj Close\"]\n",
    "\n",
    "r = np.log(IBM) - np.log(IBM.shift(1)) # Daily log return\n",
    "r = r.to_numpy()\n",
    "r = np.delete(r , 0) # Remove first date because 1 lag in returns\n",
    "\n",
    "train_X = r[0:2500]\n",
    "test_X = r[2500:3018]\n",
    "train_y = r[1:2501]\n",
    "test_y = r[2501:3019]\n",
    "\n",
    "# Construct neural network\n",
    "HIDDEN = 16\n",
    "NN =keras.Sequential()\n",
    "NN.add(keras.layers.Dense(HIDDEN, input_dim=1,activation='relu')) #1 input x\n",
    "NN.add(keras.layers.Dense(HIDDEN, activation='relu'))\n",
    "NN.add(keras.layers.Dense(1, activation='linear')) #1 output y\n",
    "\n",
    "NN.summary()\n",
    "\n",
    "# State the loss function and optimizer (adam is a good choice usually)\n",
    "NN.compile(optimizer=keras.optimizers.Adam(),loss='mean_squared_error')\n",
    "\n",
    "# Fit the model to training data\n",
    "EPOCHS = 100 # How long to train for (cut off early)\n",
    "history = NN.fit(train_X,train_y,epochs=EPOCHS,validation_split=0.1,verbose=0)\n",
    "# View improvement over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.show()\n",
    "print(history.history['loss'][-1]) # Training MSE\n",
    "print(history.history['val_loss'][-1]) # Validation loss\n",
    "\n",
    "# Evaluate on test data\n",
    "print(NN.evaluate(test_X , test_y , verbose=0)) # Test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this compare with an AR(1) model\n",
    "XX = np.vstack([np.ones(len(train_X)),train_X]).T\n",
    "b0,b1 = np.linalg.lstsq(XX,train_y,rcond=None)[0]\n",
    "\n",
    "# Compute MSE\n",
    "print(np.mean(((b0+b1*test_X) - test_y)**2))\n",
    "\n",
    "\n",
    "# How should we modify this NN to consider 2+ lags?\n",
    "# What other network structures should we try?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
